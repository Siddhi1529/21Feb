{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data.\n"
      ],
      "metadata": {
        "id": "dj8efHfAXQBh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Web scraping is an automated process of extracting data from websites. It involves using tools or scripts to retrieve and parse HTML content from web pages to gather specific information.\n",
        "\n",
        "Usage :\n",
        "1. **Data Collection**: To collect large volumes of data quickly and efficiently from websites.\n",
        "2. **Automation**: To eliminate manual data copying, saving time and reducing errors.\n",
        "3. **Analysis**: To gather data for insights, trends, or decision-making.\n",
        "\n",
        "Areas Where Web Scraping is Used:\n",
        "1. **E-commerce**: To monitor competitor prices and product availability.\n",
        "2. **Research**: To gather data for academic, scientific, or market research.\n",
        "3. **Job Portals**: To extract job listings for analysis or aggregation."
      ],
      "metadata": {
        "id": "2-OxNClnXczI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IlsLu_9EXShx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What are the different methods used for Web Scraping?\n",
        "\n"
      ],
      "metadata": {
        "id": "S9F_4y5bXSo_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Methods Used for Web Scraping:\n",
        "\n",
        "1. **Manual Scraping**:\n",
        "   - Copy-pasting data manually from websites.\n",
        "   - Suitable for small tasks but not efficient for large-scale scraping.\n",
        "\n",
        "2. **Using Libraries and Tools**:\n",
        "   - Libraries like **BeautifulSoup**, **Scrapy**, and **Selenium** in Python are used for parsing and automating web data extraction.\n",
        "\n",
        "3. **APIs**:\n",
        "   - Many websites provide APIs (e.g., Twitter API, YouTube API) for accessing structured data without scraping.\n",
        "\n",
        "4. **Browser Extensions**:\n",
        "   - Tools like **Data Miner** or **Web Scraper** extensions allow data extraction directly from browsers.\n",
        "\n",
        "5. **Custom Scripts**:\n",
        "   - Writing custom Python scripts using HTTP requests and parsing libraries like **Requests** and **BeautifulSoup**.\n",
        "\n",
        "6. **Headless Browsers**:\n",
        "   - Tools like **Selenium** simulate user behavior and interact with dynamic content or JavaScript-rendered pages."
      ],
      "metadata": {
        "id": "yfcj1h-GXrXL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4BMtDFumXXi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. What is Beautiful Soup? Why is it used?"
      ],
      "metadata": {
        "id": "TRqI6JD2XXpv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is Beautiful Soup?\n",
        "**Beautiful Soup** is a Python library used for web scraping. It allows you to parse HTML and XML documents, making it easier to navigate, search, and extract specific data.\n",
        "\n",
        "### Why is it Used?\n",
        "1. **HTML Parsing**: Extracts data from poorly structured HTML or XML documents.\n",
        "2. **Ease of Use**: Provides a simple API for searching and modifying the parse tree.\n",
        "3. **Integration**: Works seamlessly with other libraries like `requests` for fetching web pages.\n",
        "\n",
        "### Example Use Case:\n",
        "- Extracting titles, links, or tables from a webpage for data analysis or automation tasks."
      ],
      "metadata": {
        "id": "qTLyHjFAXwoa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vLvCp8IhXTf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Why is flask used in this Web Scraping project?\n"
      ],
      "metadata": {
        "id": "_PZKy8p-XTmA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why Flask is Used in a Web Scraping Project:\n",
        "1. **Web Interface**: Flask allows you to create a user-friendly web interface to display or interact with the scraped data.\n",
        "2. **API Development**: Flask can expose scraped data as an API for use by other applications or systems.\n",
        "3. **Integration**: It integrates well with scraping libraries like **Beautiful Soup** or **Scrapy**, enabling dynamic data fetching.\n",
        "4. **Lightweight and Flexible**: Flaskâ€™s simplicity and lightweight framework make it ideal for small projects like web scraping tools.\n",
        "\n",
        "### Example:\n",
        "- You can use Flask to display scraped data (e.g., product prices or news headlines) on a webpage or to provide endpoints for accessing the data programmatically."
      ],
      "metadata": {
        "id": "SUUuMMkIX1_w"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qMB7EB7dXWMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. Write the names of AWS services used in this project. Also, explain the use of each service."
      ],
      "metadata": {
        "id": "pNh14sEAXVJ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AWS Services and their sses in Web Scraping:\n",
        "\n",
        "1. **Amazon EC2**: Hosts scraping scripts and provides scalable servers.  \n",
        "2. **Amazon S3**: Stores scraped data and files securely.  \n",
        "3. **AWS Lambda**: Runs scraping scripts serverlessly, ideal for event-driven tasks.  \n",
        "4. **Amazon RDS**: Stores structured data in relational databases like MySQL.  \n",
        "5. **AWS CloudWatch**: Monitors and logs scraping performance.  \n",
        "6. **Amazon DynamoDB**: Saves unstructured data in a NoSQL format.  \n",
        "7. **AWS API Gateway**: Exposes APIs to share scraped data.  \n",
        "8. **Amazon Step Functions**: Orchestrates workflows for complex scraping jobs.  \n",
        "\n",
        "These services make scraping scalable, efficient, and manageable."
      ],
      "metadata": {
        "id": "SkhPuEoVX4WN"
      }
    }
  ]
}